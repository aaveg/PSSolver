{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "635307ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from solver import System\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce6afdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dyn = torch.randn(2,256,256, device=device)\n",
    "stat = torch.randn(4,256,256, device=device)\n",
    "combined = torch.cat([dyn, stat], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77723b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 256, 256]), torch.Size([4, 256, 256]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dyn = combined[0:2]\n",
    "stat = combined[2:]\n",
    "dyn.shape, stat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "790739d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b28ba200",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[1] = torch.zeros_like(combined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d371db22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.5678,  1.2433,  0.5886,  ..., -0.1710,  1.5331, -0.2803],\n",
       "         [-0.7151,  0.6454, -0.7509,  ...,  0.6604, -1.6871,  0.2438],\n",
       "         [ 0.4153,  0.7941,  2.1794,  ..., -0.7929,  0.2209, -0.4588],\n",
       "         ...,\n",
       "         [ 0.1401,  2.0077, -0.7475,  ..., -0.2444,  0.3581, -1.7482],\n",
       "         [ 0.9350,  1.1395,  0.1716,  ...,  2.1571,  1.1981, -0.5420],\n",
       "         [ 0.6944, -1.3484,  1.3126,  ...,  0.3894, -0.8434,  0.6086]],\n",
       "\n",
       "        [[ 0.1908, -0.3779,  1.1071,  ..., -0.4137,  2.0128, -1.7150],\n",
       "         [-0.3690,  0.7551, -0.5786,  ..., -1.2326, -0.6990,  2.2382],\n",
       "         [-0.8155, -1.3193, -0.3175,  ...,  0.8341, -0.4897, -0.7313],\n",
       "         ...,\n",
       "         [ 0.5690,  0.5264,  0.1834,  ...,  0.3936, -0.5091,  1.0592],\n",
       "         [-1.2375,  0.4327,  0.6330,  ...,  1.2925, -0.2190,  0.3895],\n",
       "         [ 0.0343, -1.5559,  0.4339,  ..., -0.5316, -0.2746, -0.1635]],\n",
       "\n",
       "        [[-0.1620, -0.9595,  1.3633,  ..., -0.7681,  0.0696, -1.3680],\n",
       "         [-0.2318,  0.4935,  1.2169,  ...,  0.7376,  0.4411, -0.8874],\n",
       "         [-0.8833, -1.7575, -0.6985,  ...,  1.3604,  0.5167, -0.2456],\n",
       "         ...,\n",
       "         [ 0.5620, -1.2742, -0.4237,  ..., -1.6146, -0.8222, -0.1642],\n",
       "         [-0.6988, -0.7360,  0.6652,  ..., -0.7962, -0.3284, -0.5163],\n",
       "         [ 0.2692,  0.2350,  1.4597,  ...,  0.1061,  0.2361,  1.5833]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a16f975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(50000):\n",
    "    # Split dyn and stat into 8 matrices each (assuming shape [4, 256, 256])\n",
    "    for d in dyn:\n",
    "        d_fft = torch.fft.fft2(d)\n",
    "        d_ifft = torch.fft.ifft2(d_fft).real\n",
    "    for s in stat:\n",
    "        s_fft = torch.fft.fft2(s)\n",
    "        s_ifft = torch.fft.ifft2(s_fft).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2032f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(50000):\n",
    "    # Split dyn and stat into 8 matrices each (assuming shape [4, 256, 256])\n",
    "    d_fft = torch.fft.fft2(dyn)\n",
    "    d_ifft = torch.fft.ifft2(d_fft).real\n",
    "    \n",
    "    s_fft = torch.fft.fft2(stat)\n",
    "    s_ifft = torch.fft.ifft2(s_fft).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c8d171b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.2804,  0.0480, -0.6264,  ...,  2.1805, -1.4438,  0.2602],\n",
       "         [-0.1609,  1.8413, -0.6910,  ...,  0.0563,  0.1463, -0.3662],\n",
       "         [-1.2921, -0.7539, -2.7910,  ..., -0.7983, -0.9346,  0.1365],\n",
       "         ...,\n",
       "         [-0.1097,  1.0264, -0.5052,  ..., -0.1165,  0.0831, -0.7865],\n",
       "         [-1.3239, -0.2640, -0.2287,  ..., -1.0634,  1.0518,  0.1197],\n",
       "         [ 0.7705,  0.1071, -0.6343,  ..., -0.3549, -0.8197,  0.2015]]),\n",
       " tensor([[ 1.2804,  0.0480, -0.6264,  ...,  2.1805, -1.4438,  0.2602],\n",
       "         [-0.1609,  1.8413, -0.6910,  ...,  0.0563,  0.1463, -0.3662],\n",
       "         [-1.2921, -0.7539, -2.7910,  ..., -0.7983, -0.9346,  0.1365],\n",
       "         ...,\n",
       "         [-0.1097,  1.0264, -0.5052,  ..., -0.1165,  0.0831, -0.7865],\n",
       "         [-1.3239, -0.2640, -0.2287,  ..., -1.0634,  1.0518,  0.1197],\n",
       "         [ 0.7705,  0.1071, -0.6343,  ..., -0.3549, -0.8197,  0.2015]]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s,dyn_ifft[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d55f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dyn.shape, dyn_fft.shape, dyn_ifft.shape, dyn == dyn_ifft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cc6a8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(50000):\n",
    "    dyn_fft = torch.fft.fft2(combined)\n",
    "    dyn_ifft = torch.fft.ifft2(dyn_fft).real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e906a3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(2, 0, 5, 5))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(2,  0,  5, 5)\n",
    "a\n",
    "# a = torch.cat([a, torch.zeros(1, 5, 5)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9878dc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77ca83e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]],\n",
       "\n",
       "        [[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_fft = torch.fft.fft2(a)\n",
    "a_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4835197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]],\n",
       "\n",
       "        [[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "         [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_fft = torch.fft.fftn(a,dim =(1,2))\n",
    "b_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7462e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from solver import PseudoSpectralSolver2D\n",
    "# import time\n",
    "\n",
    "# solver = PseudoSpectralSolver2D( N, L, dt=dt, device=device,record_every_n_steps = steps//100)\n",
    "\n",
    "\n",
    "# a = -1\n",
    "# b = 1\n",
    "# k = 4\n",
    "\n",
    "# solver.set_initial_condition(0.1 * torch.randn(N, N, device=device))\n",
    "# solver.set_linear_func(lambda qx,qy,q2: -q2 * (a + k*q2))\n",
    "# solver.set_nonlinear_func(lambda qx,qy,q2,u: -q2 * b * torch.fft.fft2(u**3))\n",
    "\n",
    "# start = time.time()\n",
    "# traj = solver.run(steps)\n",
    "# end = time.time()\n",
    "# print(f\"Elapsed time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1db4bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128\n",
    "L = 128#2 * torch.pi\n",
    "dt = 0.1\n",
    "steps = 100000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64d629aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_u(N):\n",
    "    u = torch.ones(N, N)\n",
    "    r = 10  # half size of perturbation square\n",
    "    u[N//2 - r:N//2 + r, N//2 - r:N//2 + r] = 0.50\n",
    "    u += 0.02 * torch.randn(N, N)\n",
    "    return u\n",
    "\n",
    "def init_v(N):\n",
    "    v = torch.zeros(N, N)\n",
    "    r = 10\n",
    "    v[N//2 - r:N//2 + r, N//2 - r:N//2 + r] = 0.25\n",
    "    v += 0.02 * torch.randn(N, N)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8740645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 11.414809 seconds\n",
      "torch.Size([100, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "solver = System(N=N, L=L, dt=dt, device=device, record_every_n_steps = steps/100)\n",
    "\n",
    "a = -1\n",
    "b = 1\n",
    "k = 4\n",
    "\n",
    "# # --- Add active fields ---\n",
    "# solver.model.add_active_field(\n",
    "#     \"u\",\n",
    "#     init = 0.1 * torch.randn((N, N)),\n",
    "#     L_hat = -solver.q2 * (a + k*solver.q2),\n",
    "#     N_hat = lambda R: -solver.q2 * b * torch.fft.fft2(R[\"u\"].f**3)\n",
    "# )\n",
    "\n",
    "# --- Add active fields ---\n",
    "du = 0.16\n",
    "dv = 0.08\n",
    "F = 0.06\n",
    "# F = 0.0493\n",
    "k = 0.062\n",
    "\n",
    "solver.model.add_active_field(\n",
    "    \"u\",\n",
    "    init = init_u(N),\n",
    "    L_hat = -solver.q2 * (du),\n",
    "    N_hat = lambda R: torch.fft.fft2(-R[\"u\"].f * R[\"v\"].f**2 + F * (1 - R[\"u\"].f))\n",
    ")\n",
    "# --- Add active fields ---\n",
    "solver.model.add_active_field(\n",
    "    \"v\",\n",
    "    init =  init_v(N),\n",
    "    L_hat = -solver.q2 * (dv),\n",
    "    N_hat = lambda R: torch.fft.fft2(R[\"u\"].f * R[\"v\"].f**2 - (F + k) * R[\"v\"].f)\n",
    ")\n",
    "\n",
    "traj = []\n",
    "start = time.time()\n",
    "for i in range(steps):\n",
    "    solver.run(1)\n",
    "    if i % solver.record_every_n_steps == 0:\n",
    "        traj.append(solver.model.fields['u'].f)\n",
    "end = time.time()\n",
    "print(f\"Elapsed time: {end - start:.6f} seconds\")\n",
    "traj = torch.stack(traj)\n",
    "\n",
    "# traj = solver.run(steps)\n",
    "print(traj.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb88d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(traj[0].detach().cpu().numpy(), cmap='viridis', origin='lower', animated=True)\n",
    "ax.set_title(\"PseudoSpectralSolver2D Evolution\")\n",
    "cbar = fig.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Field Value\")\n",
    "\n",
    "def update(frame):\n",
    "    im.set_array(traj[frame].detach().cpu().numpy())\n",
    "    ax.set_xlabel(f\"Step {frame}\")\n",
    "    im.set_clim(vmin=traj[frame].min().cpu().item(), vmax=traj[frame].max().cpu().item())\n",
    "    cbar.update_normal(im)\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=traj.shape[0], blit=True, interval=50)\n",
    "plt.close(fig)  # Prevents duplicate static plot\n",
    "display(HTML(ani.to_jshtml()))\n",
    "print(traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "49e1f219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.qx.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "17240dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-9559.6758, device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = traj[-1]\n",
    "z = -f.sum()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e670a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if F.grad is not None:\n",
    "    F.grad.zero_()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8d5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-102.0163, device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a8ca5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in autograd computational graph for z: 15\n"
     ]
    }
   ],
   "source": [
    "def count_torch_graph_nodes(var):\n",
    "    seen = set()\n",
    "    def traverse(node):\n",
    "        if node is None or node in seen:\n",
    "            return 0\n",
    "        seen.add(node)\n",
    "        total = 1\n",
    "        if hasattr(node, 'next_functions'):\n",
    "            for n, _ in node.next_functions:\n",
    "                total += traverse(n)\n",
    "        if hasattr(node, 'saved_tensors'):\n",
    "            for t in node.saved_tensors:\n",
    "                if hasattr(t, 'grad_fn'):\n",
    "                    total += traverse(t.grad_fn)\n",
    "        return total\n",
    "    return traverse(var.grad_fn)\n",
    "\n",
    "graph_size = count_torch_graph_nodes(z)\n",
    "print(f\"Number of nodes in autograd computational graph for z: {graph_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1877b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,\n",
      "         12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,\n",
      "         24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,\n",
      "         36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,\n",
      "         48.,  49.,  50.,  51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,\n",
      "         60.,  61.,  62.,  63., -64., -63., -62., -61., -60., -59., -58., -57.,\n",
      "        -56., -55., -54., -53., -52., -51., -50., -49., -48., -47., -46., -45.,\n",
      "        -44., -43., -42., -41., -40., -39., -38., -37., -36., -35., -34., -33.,\n",
      "        -32., -31., -30., -29., -28., -27., -26., -25., -24., -23., -22., -21.,\n",
      "        -20., -19., -18., -17., -16., -15., -14., -13., -12., -11., -10.,  -9.,\n",
      "         -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "k = torch.fft.fftfreq(N,d=1/N).to(device)\n",
    "# print(k)\n",
    "kx, ky = torch.meshgrid(k, k, indexing=\"ij\")\n",
    "# mask = ((kx.abs() < N//3) & (ky.abs() < N//3))\n",
    "\n",
    "print(k)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9833084",
   "metadata": {},
   "outputs": [],
   "source": [
    "kx = torch.fft.fftfreq(N, d=L/N).to(device) * 2 * torch.pi\n",
    "ky = torch.fft.rfftfreq(N, d=L/N).to(device) * 2 * torch.pi\n",
    "kx, ky = torch.meshgrid(kx, ky, indexing='ij')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf74c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0491,  0.0491,  0.0491,  ...,  0.0491,  0.0491,  0.0491],\n",
       "         [ 0.0982,  0.0982,  0.0982,  ...,  0.0982,  0.0982,  0.0982],\n",
       "         ...,\n",
       "         [-0.1473, -0.1473, -0.1473,  ..., -0.1473, -0.1473, -0.1473],\n",
       "         [-0.0982, -0.0982, -0.0982,  ..., -0.0982, -0.0982, -0.0982],\n",
       "         [-0.0491, -0.0491, -0.0491,  ..., -0.0491, -0.0491, -0.0491]],\n",
       "        device='cuda:0'),\n",
       " torch.Size([128, 65]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kx,kx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26e06d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m u = \u001b[43mtraj\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      2\u001b[39m u_hat = torch.fft.rfft2(u)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(u_hat.shape)\n",
      "\u001b[31mIndexError\u001b[39m: index 10 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "u = traj[10]\n",
    "u_hat = torch.fft.rfft2(u)\n",
    "print(u_hat.shape)\n",
    "un = torch.fft.irfft2(u_hat)\n",
    "print(un.shape)\n",
    "u_hat[1,1],u_hat[-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633fb585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9995, 0.9993, 0.9993,  ..., 0.9998, 0.9999, 0.9997],\n",
       "         [0.9996, 0.9998, 1.0000,  ..., 0.9991, 0.9992, 0.9994],\n",
       "         [0.9997, 1.0001, 1.0004,  ..., 0.9984, 0.9986, 0.9992],\n",
       "         ...,\n",
       "         [1.0001, 1.0002, 0.9999,  ..., 1.0004, 1.0003, 1.0001],\n",
       "         [0.9997, 0.9995, 0.9992,  ..., 1.0005, 1.0003, 0.9999],\n",
       "         [0.9995, 0.9991, 0.9989,  ..., 1.0003, 1.0003, 0.9999]],\n",
       "        device='cuda:0', grad_fn=<FftC2RBackward0>),\n",
       " tensor([[0.9995, 0.9993, 0.9993,  ..., 0.9998, 0.9999, 0.9997],\n",
       "         [0.9996, 0.9998, 1.0000,  ..., 0.9991, 0.9992, 0.9994],\n",
       "         [0.9997, 1.0001, 1.0004,  ..., 0.9984, 0.9986, 0.9992],\n",
       "         ...,\n",
       "         [1.0001, 1.0002, 0.9999,  ..., 1.0004, 1.0003, 1.0001],\n",
       "         [0.9997, 0.9995, 0.9992,  ..., 1.0005, 1.0003, 0.9999],\n",
       "         [0.9995, 0.9991, 0.9989,  ..., 1.0003, 1.0003, 0.9999]],\n",
       "        device='cuda:0', grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un,u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7621400f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraj\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlower\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextent\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33m2/3 Dealiasing Mask\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mkx\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.13/site-packages/matplotlib/pyplot.py:3592\u001b[39m, in \u001b[36mimshow\u001b[39m\u001b[34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[39m\n\u001b[32m   3570\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.imshow)\n\u001b[32m   3571\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimshow\u001b[39m(\n\u001b[32m   3572\u001b[39m     X: ArrayLike | PIL.Image.Image,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3590\u001b[39m     **kwargs,\n\u001b[32m   3591\u001b[39m ) -> AxesImage:\n\u001b[32m-> \u001b[39m\u001b[32m3592\u001b[39m     __ret = \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3594\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3595\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3596\u001b[39m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[43m=\u001b[49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3597\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3598\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3602\u001b[39m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m=\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3604\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3606\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3607\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3608\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3609\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3610\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3611\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3612\u001b[39m     sci(__ret)\n\u001b[32m   3613\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.13/site-packages/matplotlib/__init__.py:1521\u001b[39m, in \u001b[36m_preprocess_data.<locals>.inner\u001b[39m\u001b[34m(ax, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1518\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(ax, *args, data=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1521\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1526\u001b[39m     bound = new_sig.bind(ax, *args, **kwargs)\n\u001b[32m   1527\u001b[39m     auto_label = (bound.arguments.get(label_namer)\n\u001b[32m   1528\u001b[39m                   \u001b[38;5;129;01mor\u001b[39;00m bound.kwargs.get(label_namer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.13/site-packages/matplotlib/axes/_axes.py:5945\u001b[39m, in \u001b[36mAxes.imshow\u001b[39m\u001b[34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[39m\n\u001b[32m   5942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5943\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_aspect(aspect)\n\u001b[32m-> \u001b[39m\u001b[32m5945\u001b[39m \u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5946\u001b[39m im.set_alpha(alpha)\n\u001b[32m   5947\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m im.get_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5948\u001b[39m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.13/site-packages/matplotlib/image.py:675\u001b[39m, in \u001b[36m_ImageBase.set_data\u001b[39m\u001b[34m(self, A)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL.Image.Image):\n\u001b[32m    674\u001b[39m     A = pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m \u001b[38;5;28mself\u001b[39m._A = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[38;5;28mself\u001b[39m._imcache = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    677\u001b[39m \u001b[38;5;28mself\u001b[39m.stale = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.13/site-packages/matplotlib/image.py:636\u001b[39m, in \u001b[36m_ImageBase._normalize_image_array\u001b[39m\u001b[34m(A)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_normalize_image_array\u001b[39m(A):\n\u001b[32m    632\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[33;03m    Check validity of image-like input *A* and normalize it to a format suitable for\u001b[39;00m\n\u001b[32m    634\u001b[39m \u001b[33;03m    Image subclasses.\u001b[39;00m\n\u001b[32m    635\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m     A = \u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_masked_invalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m A.dtype != np.uint8 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.can_cast(A.dtype, \u001b[38;5;28mfloat\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msame_kind\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    638\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage data of dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    639\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mconverted to float\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.13/site-packages/matplotlib/cbook.py:684\u001b[39m, in \u001b[36msafe_masked_invalid\u001b[39m\u001b[34m(x, copy)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msafe_masked_invalid\u001b[39m(x, copy=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m     x = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    685\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x.dtype.isnative:\n\u001b[32m    686\u001b[39m         \u001b[38;5;66;03m# If we have already made a copy, do the byteswap in place, else make a\u001b[39;00m\n\u001b[32m    687\u001b[39m         \u001b[38;5;66;03m# copy with the byte order swapped.\u001b[39;00m\n\u001b[32m    688\u001b[39m         \u001b[38;5;66;03m# Swap to native order.\u001b[39;00m\n\u001b[32m    689\u001b[39m         x = x.byteswap(inplace=copy).view(x.dtype.newbyteorder(\u001b[33m'\u001b[39m\u001b[33mN\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.13/site-packages/torch/_tensor.py:1194\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor.__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype=dtype)\n\u001b[32m   1193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAGiCAYAAACGUJO6AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGupJREFUeJzt3XtsVGX+x/HPtEOnyG7HCFoL1Fpc0CoRlzZUylajKzVAMCS7oYYNBRcTG3UrdHGhdiNCTBrdyK631gsUYlLYRgWXP7rK/LFCueyFbmuMbaIBtEVbm5bQFnEHKc/vD9L5ObZoz9ALX/t+JeePeTxn5pkndd6cMzOtzznnBACAMXGjPQEAAGJBwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmeQ7Y/v37tXjxYk2ePFk+n0/vvPPODx6zb98+ZWZmKjExUdOmTdMrr7wSy1wBAIjwHLCvvvpKs2bN0ksvvTSo/Y8fP66FCxcqNzdX9fX1euKJJ1RUVKS3337b82QBAOjju5Rf5uvz+bR7924tWbLkovusW7dOe/bsUVNTU2SssLBQH3zwgQ4fPhzrQwMAxjj/cD/A4cOHlZeXFzV27733auvWrfrmm280bty4fseEw2GFw+HI7fPnz+vkyZOaOHGifD7fcE8ZADCEnHPq6enR5MmTFRc3dB+9GPaAtbW1KTk5OWosOTlZ586dU0dHh1JSUvodU1ZWpo0bNw731AAAI6ilpUVTp04dsvsb9oBJ6nfW1HfV8mJnUyUlJSouLo7c7urq0nXXXaeWlhYlJSUN30QBAEOuu7tbqamp+ulPfzqk9zvsAbv22mvV1tYWNdbe3i6/36+JEycOeEwgEFAgEOg3npSURMAAwKihfgto2L8HNnfuXIVCoaixvXv3Kisra8D3vwAAGAzPATt9+rQaGhrU0NAg6cLH5BsaGtTc3CzpwuW/goKCyP6FhYX67LPPVFxcrKamJlVWVmrr1q1au3bt0DwDAMCY5PkS4pEjR3TXXXdFbve9V7VixQpt375dra2tkZhJUnp6umpqarRmzRq9/PLLmjx5sl544QX96le/GoLpAwDGqkv6HthI6e7uVjAYVFdXF++BAYAxw/Uazu9CBACYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASTEFrLy8XOnp6UpMTFRmZqZqa2u/d/+qqirNmjVLV1xxhVJSUvTAAw+os7MzpgkDACDFELDq6mqtXr1apaWlqq+vV25urhYsWKDm5uYB9z9w4IAKCgq0atUqffTRR3rzzTf1n//8Rw8++OAlTx4AMHZ5DtjmzZu1atUqPfjgg8rIyNBf/vIXpaamqqKiYsD9//nPf+r6669XUVGR0tPT9Ytf/EIPPfSQjhw5csmTBwCMXZ4CdvbsWdXV1SkvLy9qPC8vT4cOHRrwmJycHJ04cUI1NTVyzunLL7/UW2+9pUWLFl30ccLhsLq7u6M2AAC+zVPAOjo61Nvbq+Tk5Kjx5ORktbW1DXhMTk6OqqqqlJ+fr4SEBF177bW68sor9eKLL170ccrKyhQMBiNbamqql2kCAMaAmD7E4fP5om475/qN9WlsbFRRUZGefPJJ1dXV6d1339Xx48dVWFh40fsvKSlRV1dXZGtpaYllmgCAHzG/l50nTZqk+Pj4fmdb7e3t/c7K+pSVlWnevHl6/PHHJUm33nqrJkyYoNzcXD399NNKSUnpd0wgEFAgEPAyNQDAGOPpDCwhIUGZmZkKhUJR46FQSDk5OQMec+bMGcXFRT9MfHy8pAtnbgAAxMLzJcTi4mJt2bJFlZWVampq0po1a9Tc3By5JFhSUqKCgoLI/osXL9auXbtUUVGhY8eO6eDBgyoqKtKcOXM0efLkoXsmAIAxxdMlREnKz89XZ2enNm3apNbWVs2cOVM1NTVKS0uTJLW2tkZ9J2zlypXq6enRSy+9pN///ve68sordffdd+uZZ54ZumcBABhzfM7Adbzu7m4Fg0F1dXUpKSlptKcDAPBguF7D+V2IAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwKaaAlZeXKz09XYmJicrMzFRtbe337h8Oh1VaWqq0tDQFAgHdcMMNqqysjGnCAABIkt/rAdXV1Vq9erXKy8s1b948vfrqq1qwYIEaGxt13XXXDXjM0qVL9eWXX2rr1q362c9+pvb2dp07d+6SJw8AGLt8zjnn5YDs7GzNnj1bFRUVkbGMjAwtWbJEZWVl/fZ/9913df/99+vYsWO66qqrYppkd3e3gsGgurq6lJSUFNN9AABGx3C9hnu6hHj27FnV1dUpLy8vajwvL0+HDh0a8Jg9e/YoKytLzz77rKZMmaIZM2Zo7dq1+vrrry/6OOFwWN3d3VEbAADf5ukSYkdHh3p7e5WcnBw1npycrLa2tgGPOXbsmA4cOKDExETt3r1bHR0devjhh3Xy5MmLvg9WVlamjRs3epkaAGCMielDHD6fL+q2c67fWJ/z58/L5/OpqqpKc+bM0cKFC7V582Zt3779omdhJSUl6urqimwtLS2xTBMA8CPm6Qxs0qRJio+P73e21d7e3u+srE9KSoqmTJmiYDAYGcvIyJBzTidOnND06dP7HRMIBBQIBLxMDQAwxng6A0tISFBmZqZCoVDUeCgUUk5OzoDHzJs3T1988YVOnz4dGfv4448VFxenqVOnxjBlAABiuIRYXFysLVu2qLKyUk1NTVqzZo2am5tVWFgo6cLlv4KCgsj+y5Yt08SJE/XAAw+osbFR+/fv1+OPP67f/va3Gj9+/NA9EwDAmOL5e2D5+fnq7OzUpk2b1NraqpkzZ6qmpkZpaWmSpNbWVjU3N0f2/8lPfqJQKKTf/e53ysrK0sSJE7V06VI9/fTTQ/csAABjjufvgY0GvgcGAHZdFt8DAwDgckHAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkxBay8vFzp6elKTExUZmamamtrB3XcwYMH5ff7ddttt8XysAAARHgOWHV1tVavXq3S0lLV19crNzdXCxYsUHNz8/ce19XVpYKCAv3yl7+MebIAAPTxOeeclwOys7M1e/ZsVVRURMYyMjK0ZMkSlZWVXfS4+++/X9OnT1d8fLzeeecdNTQ0XHTfcDiscDgcud3d3a3U1FR1dXUpKSnJy3QBAKOsu7tbwWBwyF/DPZ2BnT17VnV1dcrLy4saz8vL06FDhy563LZt23T06FFt2LBhUI9TVlamYDAY2VJTU71MEwAwBngKWEdHh3p7e5WcnBw1npycrLa2tgGP+eSTT7R+/XpVVVXJ7/cP6nFKSkrU1dUV2VpaWrxMEwAwBgyuKN/h8/mibjvn+o1JUm9vr5YtW6aNGzdqxowZg77/QCCgQCAQy9QAAGOEp4BNmjRJ8fHx/c622tvb+52VSVJPT4+OHDmi+vp6Pfroo5Kk8+fPyzknv9+vvXv36u67776E6QMAxipPlxATEhKUmZmpUCgUNR4KhZSTk9Nv/6SkJH344YdqaGiIbIWFhbrxxhvV0NCg7OzsS5s9AGDM8nwJsbi4WMuXL1dWVpbmzp2r1157Tc3NzSosLJR04f2rzz//XG+88Ybi4uI0c+bMqOOvueYaJSYm9hsHAMALzwHLz89XZ2enNm3apNbWVs2cOVM1NTVKS0uTJLW2tv7gd8IAALhUnr8HNhqG6zsEAIDhd1l8DwwAgMsFAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmxRSw8vJypaenKzExUZmZmaqtrb3ovrt27dL8+fN19dVXKykpSXPnztV7770X84QBAJBiCFh1dbVWr16t0tJS1dfXKzc3VwsWLFBzc/OA++/fv1/z589XTU2N6urqdNddd2nx4sWqr6+/5MkDAMYun3POeTkgOztbs2fPVkVFRWQsIyNDS5YsUVlZ2aDu45ZbblF+fr6efPLJAf97OBxWOByO3O7u7lZqaqq6urqUlJTkZboAgFHW3d2tYDA45K/hns7Azp49q7q6OuXl5UWN5+Xl6dChQ4O6j/Pnz6unp0dXXXXVRfcpKytTMBiMbKmpqV6mCQAYAzwFrKOjQ729vUpOTo4aT05OVltb26Du47nnntNXX32lpUuXXnSfkpISdXV1RbaWlhYv0wQAjAH+WA7y+XxRt51z/cYGsnPnTj311FP629/+pmuuueai+wUCAQUCgVimBgAYIzwFbNKkSYqPj+93ttXe3t7vrOy7qqurtWrVKr355pu65557vM8UAIBv8XQJMSEhQZmZmQqFQlHjoVBIOTk5Fz1u586dWrlypXbs2KFFixbFNlMAAL7F8yXE4uJiLV++XFlZWZo7d65ee+01NTc3q7CwUNKF968+//xzvfHGG5IuxKugoEDPP/+8br/99sjZ2/jx4xUMBofwqQAAxhLPAcvPz1dnZ6c2bdqk1tZWzZw5UzU1NUpLS5Mktba2Rn0n7NVXX9W5c+f0yCOP6JFHHomMr1ixQtu3b7/0ZwAAGJM8fw9sNAzXdwgAAMPvsvgeGAAAlwsCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEyKKWDl5eVKT09XYmKiMjMzVVtb+73779u3T5mZmUpMTNS0adP0yiuvxDRZAAD6eA5YdXW1Vq9erdLSUtXX1ys3N1cLFixQc3PzgPsfP35cCxcuVG5ururr6/XEE0+oqKhIb7/99iVPHgAwdvmcc87LAdnZ2Zo9e7YqKioiYxkZGVqyZInKysr67b9u3Trt2bNHTU1NkbHCwkJ98MEHOnz48ICPEQ6HFQ6HI7e7urp03XXXqaWlRUlJSV6mCwAYZd3d3UpNTdWpU6cUDAaH7o6dB+Fw2MXHx7tdu3ZFjRcVFbk77rhjwGNyc3NdUVFR1NiuXbuc3+93Z8+eHfCYDRs2OElsbGxsbD+i7ejRo16S84P88qCjo0O9vb1KTk6OGk9OTlZbW9uAx7S1tQ24/7lz59TR0aGUlJR+x5SUlKi4uDhy+9SpU0pLS1Nzc/PQ1vtHpu9fOZypfj/WaXBYp8FhnX5Y31W0q666akjv11PA+vh8vqjbzrl+Yz+0/0DjfQKBgAKBQL/xYDDID8ggJCUlsU6DwDoNDus0OKzTD4uLG9oPvnu6t0mTJik+Pr7f2VZ7e3u/s6w+11577YD7+/1+TZw40eN0AQC4wFPAEhISlJmZqVAoFDUeCoWUk5Mz4DFz587tt//evXuVlZWlcePGeZwuAAAXeD6fKy4u1pYtW1RZWammpiatWbNGzc3NKiwslHTh/auCgoLI/oWFhfrss89UXFyspqYmVVZWauvWrVq7du2gHzMQCGjDhg0DXlbE/2OdBod1GhzWaXBYpx82XGvk+WP00oUvMj/77LNqbW3VzJkz9ec//1l33HGHJGnlypX69NNP9f7770f237dvn9asWaOPPvpIkydP1rp16yLBAwAgFjEFDACA0cbvQgQAmETAAAAmETAAgEkEDABg0mUTMP5Ey+B4Waddu3Zp/vz5uvrqq5WUlKS5c+fqvffeG8HZjg6vP0t9Dh48KL/fr9tuu214J3iZ8LpO4XBYpaWlSktLUyAQ0A033KDKysoRmu3o8bpOVVVVmjVrlq644gqlpKTogQceUGdn5wjNdnTs379fixcv1uTJk+Xz+fTOO+/84DFD8ho+pL9ZMUZ//etf3bhx49zrr7/uGhsb3WOPPeYmTJjgPvvsswH3P3bsmLviiivcY4895hobG93rr7/uxo0b5956660RnvnI8rpOjz32mHvmmWfcv//9b/fxxx+7kpISN27cOPff//53hGc+cryuUZ9Tp065adOmuby8PDdr1qyRmewoimWd7rvvPpedne1CoZA7fvy4+9e//uUOHjw4grMeeV7Xqba21sXFxbnnn3/eHTt2zNXW1rpbbrnFLVmyZIRnPrJqampcaWmpe/vtt50kt3v37u/df6hewy+LgM2ZM8cVFhZGjd10001u/fr1A+7/hz/8wd10001RYw899JC7/fbbh22OlwOv6zSQm2++2W3cuHGop3bZiHWN8vPz3R//+Ee3YcOGMREwr+v097//3QWDQdfZ2TkS07tseF2nP/3pT27atGlRYy+88IKbOnXqsM3xcjOYgA3Va/ioX0I8e/as6urqlJeXFzWel5enQ4cODXjM4cOH++1/77336siRI/rmm2+Gba6jKZZ1+q7z58+rp6dnyH8j9OUi1jXatm2bjh49qg0bNgz3FC8LsazTnj17lJWVpWeffVZTpkzRjBkztHbtWn399dcjMeVREcs65eTk6MSJE6qpqZFzTl9++aXeeustLVq0aCSmbMZQvYbH9Nvoh9JI/YkW62JZp+967rnn9NVXX2np0qXDMcVRF8saffLJJ1q/fr1qa2vl94/6/w4jIpZ1OnbsmA4cOKDExETt3r1bHR0devjhh3Xy5Mkf7ftgsaxTTk6OqqqqlJ+fr//97386d+6c7rvvPr344osjMWUzhuo1fNTPwPoM959o+bHwuk59du7cqaeeekrV1dW65pprhmt6l4XBrlFvb6+WLVumjRs3asaMGSM1vcuGl5+l8+fPy+fzqaqqSnPmzNHChQu1efNmbd++/Ud9FiZ5W6fGxkYVFRXpySefVF1dnd59910dP36cX503gKF4DR/1f3LyJ1oGJ5Z16lNdXa1Vq1bpzTff1D333DOc0xxVXteop6dHR44cUX19vR599FFJF16onXPy+/3au3ev7r777hGZ+0iK5WcpJSVFU6ZMifqDshkZGXLO6cSJE5o+ffqwznk0xLJOZWVlmjdvnh5//HFJ0q233qoJEyYoNzdXTz/99I/y6lAshuo1fNTPwPgTLYMTyzpJF868Vq5cqR07dvzor8N7XaOkpCR9+OGHamhoiGyFhYW68cYb1dDQoOzs7JGa+oiK5Wdp3rx5+uKLL3T69OnI2Mcff6y4uDhNnTp1WOc7WmJZpzNnzvT7o43x8fGS/v8MA0P4Gu7pIx/DpO+jqlu3bnWNjY1u9erVbsKECe7TTz91zjm3fv16t3z58sj+fR/BXLNmjWtsbHRbt24dUx+jH+w67dixw/n9fvfyyy+71tbWyHbq1KnRegrDzusafddY+RSi13Xq6elxU6dOdb/+9a/dRx995Pbt2+emT5/uHnzwwdF6CiPC6zpt27bN+f1+V15e7o4ePeoOHDjgsrKy3Jw5c0brKYyInp4eV19f7+rr650kt3nzZldfXx/5usFwvYZfFgFzzrmXX37ZpaWluYSEBDd79my3b9++yH9bsWKFu/POO6P2f//9993Pf/5zl5CQ4K6//npXUVExwjMeHV7W6c4773SS+m0rVqwY+YmPIK8/S982VgLmnPd1ampqcvfcc48bP368mzp1qisuLnZnzpwZ4VmPPK/r9MILL7ibb77ZjR8/3qWkpLjf/OY37sSJEyM865H1j3/843tfa4brNZw/pwIAMGnU3wMDACAWBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJj0f71QTm8PVXcxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(traj[10].cpu(), origin=\"lower\", extent=[-0.5, 0.5, -0.5, 0.5])\n",
    "plt.title(\"2/3 Dealiasing Mask\")\n",
    "plt.xlabel(\"kx\")\n",
    "plt.ylabel(\"ky\")\n",
    "plt.colorbar(label=\"Mask Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d09b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'name': \"aaveg\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1bc5bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'aaveg'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3a9397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.get('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79ebfad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba726df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
